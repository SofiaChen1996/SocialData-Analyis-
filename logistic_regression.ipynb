{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.737062937062937\n",
      "Precision: 0.7384085213032581\n",
      "Recall: 0.7354394786722434\n",
      "F1 Score: 0.7356102089627392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sofia_Chen\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#lexical based logistic regressoion \n",
    "# Load train and test sets\n",
    "train_df = pd.read_csv('C://Users//Sofia_Chen//Desktop//ds//lg_train_a.csv')\n",
    "test_df = pd.read_csv('C://Users//Sofia_Chen//Desktop//ds//lg_test_a.csv')\n",
    "\n",
    "# Prepare input features (X) and output labels (y)\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature  Importance  Absolute Importance\n",
      "101  lex_dal_avg_pleasantness   -2.027974             2.027974\n",
      "96   lex_dal_min_pleasantness   -1.496567             1.496567\n",
      "102                 sentiment   -1.365752             1.365752\n",
      "100       lex_dal_avg_imagery    0.909842             0.909842\n",
      "97     lex_dal_min_activation   -0.581958             0.581958\n",
      "95        lex_dal_max_imagery    0.493406             0.493406\n",
      "93   lex_dal_max_pleasantness   -0.419177             0.419177\n",
      "50              lex_liwc_feel    0.387585             0.387585\n",
      "74             lex_liwc_death    0.376859             0.376859\n",
      "98        lex_dal_min_imagery   -0.361524             0.361524\n",
      "94     lex_dal_max_activation    0.298280             0.298280\n",
      "31            lex_liwc_negemo   -0.285311             0.285311\n",
      "29            lex_liwc_affect    0.272897             0.272897\n",
      "76             lex_liwc_swear    0.252556             0.252556\n",
      "61              lex_liwc_risk    0.233323             0.233323\n",
      "30            lex_liwc_posemo   -0.232195             0.232195\n",
      "49              lex_liwc_hear    0.228322             0.228322\n",
      "47           lex_liwc_percept   -0.220771             0.220771\n",
      "55            lex_liwc_ingest   -0.195297             0.195297\n",
      "9            lex_liwc_pronoun   -0.192050             0.192050\n",
      "17           lex_liwc_article   -0.171989             0.171989\n",
      "80            lex_liwc_filler   -0.166847             0.166847\n",
      "19           lex_liwc_auxverb   -0.161442             0.161442\n",
      "46            lex_liwc_differ   -0.160525             0.160525\n",
      "8           lex_liwc_function    0.157077             0.157077\n",
      "86             lex_liwc_QMark    0.156185             0.156185\n",
      "32               lex_liwc_anx    0.153577             0.153577\n",
      "79            lex_liwc_nonflu    0.147847             0.147847\n",
      "90           lex_liwc_Apostro    0.140063             0.140063\n",
      "75          lex_liwc_informal   -0.121777             0.121777\n",
      "71              lex_liwc_home    0.121695             0.121695\n",
      "48               lex_liwc_see    0.121479             0.121479\n",
      "37            lex_liwc_friend    0.121134             0.121134\n",
      "33             lex_liwc_anger    0.118011             0.118011\n",
      "34               lex_liwc_sad    0.114699             0.114699\n",
      "85             lex_liwc_SemiC   -0.107849             0.107849\n",
      "18              lex_liwc_prep   -0.101838             0.101838\n",
      "59             lex_liwc_power    0.101675             0.101675\n",
      "21              lex_liwc_conj   -0.094851             0.094851\n",
      "54            lex_liwc_sexual    0.087764             0.087764\n",
      "               Feature  Importance  Absolute Importance\n",
      "89      lex_liwc_Quote   -0.026710             0.026710\n",
      "5         lex_liwc_WPS    0.025489             0.025489\n",
      "35     lex_liwc_social   -0.024199             0.024199\n",
      "91    lex_liwc_Parenth   -0.024146             0.024146\n",
      "88       lex_liwc_Dash   -0.021408             0.021408\n",
      "13        lex_liwc_you   -0.021208             0.021208\n",
      "81    lex_liwc_AllPunc   -0.019794             0.019794\n",
      "36     lex_liwc_family    0.019787             0.019787\n",
      "53     lex_liwc_health    0.018805             0.018805\n",
      "66     lex_liwc_motion   -0.017055             0.017055\n",
      "16      lex_liwc_ipron    0.016696             0.016696\n",
      "2       lex_liwc_Clout   -0.016231             0.016231\n",
      "42      lex_liwc_cause   -0.014376             0.014376\n",
      "4        lex_liwc_Tone   -0.011749             0.011749\n",
      "38     lex_liwc_female   -0.011738             0.011738\n",
      "27     lex_liwc_number    0.011340             0.011340\n",
      "65    lex_liwc_relativ    0.008965             0.008965\n",
      "58    lex_liwc_achieve    0.007030             0.007030\n",
      "68       lex_liwc_time   -0.006503             0.006503\n",
      "14      lex_liwc_shehe   -0.005150             0.005150\n",
      "92     lex_liwc_OtherP   -0.004958             0.004958\n",
      "1    lex_liwc_Analytic   -0.004657             0.004657\n",
      "24        lex_liwc_adj   -0.004603             0.004603\n",
      "0          lex_liwc_WC    0.004150             0.004150\n",
      "72      lex_liwc_money    0.001559             0.001559\n",
      "23       lex_liwc_verb    0.001392             0.001392\n",
      "6      lex_liwc_Sixltr   -0.000780             0.000780\n",
      "3   lex_liwc_Authentic    0.000737             0.000737\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Access the model's coefficients\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame of features and their coefficients\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': coefficients})\n",
    "\n",
    "# Calculate the absolute values to determine the impact\n",
    "feature_importance['Absolute Importance'] = feature_importance['Importance'].abs()\n",
    "\n",
    "# Sort by absolute importance to see the most influential features\n",
    "feature_importance = feature_importance.sort_values(by='Absolute Importance', ascending=False)\n",
    "print(feature_importance[:40])\n",
    "print(feature_importance[75:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Feature  Importance  Absolute Importance\n",
      "89      lex_liwc_Quote   -0.026710             0.026710\n",
      "5         lex_liwc_WPS    0.025489             0.025489\n",
      "35     lex_liwc_social   -0.024199             0.024199\n",
      "91    lex_liwc_Parenth   -0.024146             0.024146\n",
      "88       lex_liwc_Dash   -0.021408             0.021408\n",
      "13        lex_liwc_you   -0.021208             0.021208\n",
      "81    lex_liwc_AllPunc   -0.019794             0.019794\n",
      "36     lex_liwc_family    0.019787             0.019787\n",
      "53     lex_liwc_health    0.018805             0.018805\n",
      "66     lex_liwc_motion   -0.017055             0.017055\n",
      "16      lex_liwc_ipron    0.016696             0.016696\n",
      "2       lex_liwc_Clout   -0.016231             0.016231\n",
      "42      lex_liwc_cause   -0.014376             0.014376\n",
      "4        lex_liwc_Tone   -0.011749             0.011749\n",
      "38     lex_liwc_female   -0.011738             0.011738\n",
      "27     lex_liwc_number    0.011340             0.011340\n",
      "65    lex_liwc_relativ    0.008965             0.008965\n",
      "58    lex_liwc_achieve    0.007030             0.007030\n",
      "68       lex_liwc_time   -0.006503             0.006503\n",
      "14      lex_liwc_shehe   -0.005150             0.005150\n",
      "92     lex_liwc_OtherP   -0.004958             0.004958\n",
      "1    lex_liwc_Analytic   -0.004657             0.004657\n",
      "24        lex_liwc_adj   -0.004603             0.004603\n",
      "0          lex_liwc_WC    0.004150             0.004150\n",
      "72      lex_liwc_money    0.001559             0.001559\n",
      "23       lex_liwc_verb    0.001392             0.001392\n",
      "6      lex_liwc_Sixltr   -0.000780             0.000780\n",
      "3   lex_liwc_Authentic    0.000737             0.000737\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance[75:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7398601398601399\n",
      "Precision: 0.741243932616351\n",
      "Recall: 0.7382395789275812\n",
      "F1 Score: 0.7384228663141994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sofia_Chen\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Lexical + Syntatic \n",
    "# Load train and test sets\n",
    "train_df = pd.read_csv('C://Users//Sofia_Chen//Desktop//ds//lg_train_b.csv')\n",
    "test_df = pd.read_csv('C://Users//Sofia_Chen//Desktop//ds//lg_test_b.csv')\n",
    "\n",
    "# Prepare input features (X) and output labels (y)\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature  Importance  Absolute Importance\n",
      "102  lex_dal_avg_pleasantness   -1.994566             1.994566\n",
      "97   lex_dal_min_pleasantness   -1.524426             1.524426\n",
      "104                 sentiment   -1.405589             1.405589\n",
      "101       lex_dal_avg_imagery    0.884404             0.884404\n",
      "98     lex_dal_min_activation   -0.586494             0.586494\n",
      "96        lex_dal_max_imagery    0.497919             0.497919\n",
      "94   lex_dal_max_pleasantness   -0.432344             0.432344\n",
      "51              lex_liwc_feel    0.389339             0.389339\n",
      "75             lex_liwc_death    0.377854             0.377854\n",
      "99        lex_dal_min_imagery   -0.355434             0.355434\n",
      "95     lex_dal_max_activation    0.318411             0.318411\n",
      "32            lex_liwc_negemo   -0.290051             0.290051\n",
      "30            lex_liwc_affect    0.276094             0.276094\n",
      "77             lex_liwc_swear    0.255213             0.255213\n",
      "31            lex_liwc_posemo   -0.234808             0.234808\n",
      "62              lex_liwc_risk    0.233883             0.233883\n",
      "50              lex_liwc_hear    0.230425             0.230425\n",
      "48           lex_liwc_percept   -0.223057             0.223057\n",
      "56            lex_liwc_ingest   -0.195602             0.195602\n",
      "10           lex_liwc_pronoun   -0.190866             0.190866\n",
      "18           lex_liwc_article   -0.173058             0.173058\n",
      "20           lex_liwc_auxverb   -0.162328             0.162328\n",
      "9           lex_liwc_function    0.158415             0.158415\n",
      "47            lex_liwc_differ   -0.157418             0.157418\n",
      "33               lex_liwc_anx    0.154684             0.154684\n",
      "87             lex_liwc_QMark    0.154629             0.154629\n",
      "81            lex_liwc_filler   -0.152973             0.152973\n",
      "80            lex_liwc_nonflu    0.147349             0.147349\n",
      "91           lex_liwc_Apostro    0.141856             0.141856\n",
      "76          lex_liwc_informal   -0.125548             0.125548\n",
      "49               lex_liwc_see    0.124422             0.124422\n",
      "72              lex_liwc_home    0.122281             0.122281\n",
      "34             lex_liwc_anger    0.120581             0.120581\n",
      "38            lex_liwc_friend    0.119849             0.119849\n",
      "35               lex_liwc_sad    0.117099             0.117099\n",
      "86             lex_liwc_SemiC   -0.106581             0.106581\n",
      "19              lex_liwc_prep   -0.101931             0.101931\n",
      "60             lex_liwc_power    0.101562             0.101562\n",
      "22              lex_liwc_conj   -0.096646             0.096646\n",
      "88            lex_liwc_Exclam   -0.088288             0.088288\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Access the model's coefficients\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame of features and their coefficients\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': coefficients})\n",
    "\n",
    "# Calculate the absolute values to determine the impact\n",
    "feature_importance['Absolute Importance'] = feature_importance['Importance'].abs()\n",
    "\n",
    "# Sort by absolute importance to see the most influential features\n",
    "feature_importance = feature_importance.sort_values(by='Absolute Importance', ascending=False)\n",
    "print(feature_importance[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  Importance  Absolute Importance\n",
      "103     syntax_fk_grade   -0.020857             0.020857\n",
      "82     lex_liwc_AllPunc   -0.020636             0.020636\n",
      "89        lex_liwc_Dash   -0.020159             0.020159\n",
      "37      lex_liwc_family    0.019881             0.019881\n",
      "54      lex_liwc_health    0.018362             0.018362\n",
      "67      lex_liwc_motion   -0.017543             0.017543\n",
      "3        lex_liwc_Clout   -0.016334             0.016334\n",
      "17       lex_liwc_ipron    0.013578             0.013578\n",
      "5         lex_liwc_Tone   -0.011689             0.011689\n",
      "28      lex_liwc_number    0.011573             0.011573\n",
      "39      lex_liwc_female   -0.011198             0.011198\n",
      "43       lex_liwc_cause   -0.009850             0.009850\n",
      "66     lex_liwc_relativ    0.009667             0.009667\n",
      "59     lex_liwc_achieve    0.007827             0.007827\n",
      "15       lex_liwc_shehe   -0.006846             0.006846\n",
      "69        lex_liwc_time   -0.006586             0.006586\n",
      "0            syntax_ari    0.005613             0.005613\n",
      "93      lex_liwc_OtherP   -0.004945             0.004945\n",
      "2     lex_liwc_Analytic   -0.004724             0.004724\n",
      "1           lex_liwc_WC    0.004479             0.004479\n",
      "25         lex_liwc_adj   -0.004184             0.004184\n",
      "73       lex_liwc_money    0.001921             0.001921\n",
      "7       lex_liwc_Sixltr    0.001692             0.001692\n",
      "4    lex_liwc_Authentic    0.000576             0.000576\n",
      "24        lex_liwc_verb    0.000308             0.000308\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7454545454545455\n",
      "Precision: 0.7462859355491285\n",
      "Recall: 0.7441099989034572\n",
      "F1 Score: 0.7443498829113426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sofia_Chen\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Lexical + Syntatic + Social \n",
    "# Load train and test sets\n",
    "train_df = pd.read_csv('C://Users//Sofia_Chen//Desktop//ds//lg_train_c.csv')\n",
    "test_df = pd.read_csv('C://Users//Sofia_Chen//Desktop//ds//lg_test_c.csv')\n",
    "\n",
    "# Prepare input features (X) and output labels (y)\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Access the model's coefficients\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Create a DataFrame of features and their coefficients\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': coefficients})\n",
    "\n",
    "# Calculate the absolute values to determine the impact\n",
    "feature_importance['Absolute Importance'] = feature_importance['Importance'].abs()\n",
    "\n",
    "# Sort by absolute importance to see the most influential features\n",
    "feature_importance = feature_importance.sort_values(by='Absolute Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature  Importance  Absolute Importance\n",
      "102  lex_dal_avg_pleasantness   -1.994566             1.994566\n",
      "97   lex_dal_min_pleasantness   -1.524426             1.524426\n",
      "104                 sentiment   -1.405589             1.405589\n",
      "101       lex_dal_avg_imagery    0.884404             0.884404\n",
      "98     lex_dal_min_activation   -0.586494             0.586494\n",
      "96        lex_dal_max_imagery    0.497919             0.497919\n",
      "94   lex_dal_max_pleasantness   -0.432344             0.432344\n",
      "51              lex_liwc_feel    0.389339             0.389339\n",
      "75             lex_liwc_death    0.377854             0.377854\n",
      "99        lex_dal_min_imagery   -0.355434             0.355434\n",
      "95     lex_dal_max_activation    0.318411             0.318411\n",
      "32            lex_liwc_negemo   -0.290051             0.290051\n",
      "30            lex_liwc_affect    0.276094             0.276094\n",
      "77             lex_liwc_swear    0.255213             0.255213\n",
      "31            lex_liwc_posemo   -0.234808             0.234808\n",
      "62              lex_liwc_risk    0.233883             0.233883\n",
      "50              lex_liwc_hear    0.230425             0.230425\n",
      "48           lex_liwc_percept   -0.223057             0.223057\n",
      "56            lex_liwc_ingest   -0.195602             0.195602\n",
      "10           lex_liwc_pronoun   -0.190866             0.190866\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  Importance  Absolute Importance\n",
      "103     syntax_fk_grade   -0.020857             0.020857\n",
      "82     lex_liwc_AllPunc   -0.020636             0.020636\n",
      "89        lex_liwc_Dash   -0.020159             0.020159\n",
      "37      lex_liwc_family    0.019881             0.019881\n",
      "54      lex_liwc_health    0.018362             0.018362\n",
      "67      lex_liwc_motion   -0.017543             0.017543\n",
      "3        lex_liwc_Clout   -0.016334             0.016334\n",
      "17       lex_liwc_ipron    0.013578             0.013578\n",
      "5         lex_liwc_Tone   -0.011689             0.011689\n",
      "28      lex_liwc_number    0.011573             0.011573\n",
      "39      lex_liwc_female   -0.011198             0.011198\n",
      "43       lex_liwc_cause   -0.009850             0.009850\n",
      "66     lex_liwc_relativ    0.009667             0.009667\n",
      "59     lex_liwc_achieve    0.007827             0.007827\n",
      "15       lex_liwc_shehe   -0.006846             0.006846\n",
      "69        lex_liwc_time   -0.006586             0.006586\n",
      "0            syntax_ari    0.005613             0.005613\n",
      "93      lex_liwc_OtherP   -0.004945             0.004945\n",
      "2     lex_liwc_Analytic   -0.004724             0.004724\n",
      "1           lex_liwc_WC    0.004479             0.004479\n",
      "25         lex_liwc_adj   -0.004184             0.004184\n",
      "73       lex_liwc_money    0.001921             0.001921\n",
      "7       lex_liwc_Sixltr    0.001692             0.001692\n",
      "4    lex_liwc_Authentic    0.000576             0.000576\n",
      "24        lex_liwc_verb    0.000308             0.000308\n"
     ]
    }
   ],
   "source": [
    "print(feature_importance[80:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7454545454545455\n",
      "Precision: 0.7459468197517625\n",
      "Recall: 0.7442901452135908\n",
      "F1 Score: 0.7445305628847845\n"
     ]
    }
   ],
   "source": [
    "# Assuming feature_importance is your DataFrame containing features and their importance\n",
    "top_features = feature_importance.nlargest(18, 'Absolute Importance')['Feature']\n",
    "\n",
    "# Create a new training set using only the top ten features\n",
    "X_train_top = X_train[top_features]\n",
    "X_test_top = X_test[top_features]\n",
    "\n",
    "# Initialize and train a new logistic regression model\n",
    "model_top = LogisticRegression(max_iter=1000)\n",
    "model_top.fit(X_train_top, y_train)\n",
    "\n",
    "# Predict on the test set with the top features\n",
    "y_pred_top = model_top.predict(X_test_top)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_top)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_top, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_top, average='macro')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_top, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "71/71 [==============================] - 1s 5ms/step - loss: 0.5779 - accuracy: 0.6941 - val_loss: 0.4908 - val_accuracy: 0.7756\n",
      "Epoch 2/10\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.4714 - accuracy: 0.7692 - val_loss: 0.4825 - val_accuracy: 0.7827\n",
      "Epoch 3/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.7958 - val_loss: 0.4812 - val_accuracy: 0.7792\n",
      "Epoch 4/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8073 - val_loss: 0.4891 - val_accuracy: 0.7633\n",
      "Epoch 5/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8196 - val_loss: 0.4916 - val_accuracy: 0.7703\n",
      "Epoch 6/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8404 - val_loss: 0.5010 - val_accuracy: 0.7686\n",
      "Epoch 7/10\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8541 - val_loss: 0.5053 - val_accuracy: 0.7686\n",
      "Epoch 8/10\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.3077 - accuracy: 0.8722 - val_loss: 0.5219 - val_accuracy: 0.7686\n",
      "Epoch 9/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8899 - val_loss: 0.5303 - val_accuracy: 0.7721\n",
      "Epoch 10/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.9054 - val_loss: 0.5476 - val_accuracy: 0.7615\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5800 - accuracy: 0.7231\n",
      "Test accuracy: 0.7230769395828247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # Use 'softmax' for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "71/71 [==============================] - 1s 5ms/step - loss: 0.6153 - accuracy: 0.6538 - val_loss: 0.5647 - val_accuracy: 0.7350\n",
      "Epoch 2/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5600 - accuracy: 0.7144 - val_loss: 0.5511 - val_accuracy: 0.7244\n",
      "Epoch 3/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7237 - val_loss: 0.5485 - val_accuracy: 0.7279\n",
      "Epoch 4/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7237 - val_loss: 0.5571 - val_accuracy: 0.7367\n",
      "Epoch 5/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7263 - val_loss: 0.5494 - val_accuracy: 0.7279\n",
      "Epoch 6/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7321 - val_loss: 0.5528 - val_accuracy: 0.7244\n",
      "Epoch 7/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7339 - val_loss: 0.5516 - val_accuracy: 0.7314\n",
      "Epoch 8/10\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.5215 - accuracy: 0.7370 - val_loss: 0.5514 - val_accuracy: 0.7297\n",
      "Epoch 9/10\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7401 - val_loss: 0.5531 - val_accuracy: 0.7226\n",
      "Epoch 10/10\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7436 - val_loss: 0.5561 - val_accuracy: 0.7297\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5251 - accuracy: 0.7287\n",
      "Test accuracy: 0.7286713123321533\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_top)\n",
    "X_test_scaled = scaler.transform(X_test_top)\n",
    "\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # Use 'softmax' for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
